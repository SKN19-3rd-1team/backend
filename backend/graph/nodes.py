# backend/graph/nodes.py
"""
LangGraph ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.

ReAct íŒ¨í„´: LLMì´ ììœ¨ì ìœ¼ë¡œ tool í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì • (agent_node, should_continue)
"""

import re
from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage, AIMessage
from langgraph.prebuilt import ToolNode
from langgraph.constants import END

from .state import MentorState
from backend.rag.retriever import (
    search_major_docs,
    aggregate_major_scores,
)
from backend.rag.embeddings import get_embeddings

from backend.rag.tools import (
    list_departments,
    get_universities_by_department,
    get_major_career_info,
    get_search_help,
)

from backend.config import get_llm

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (.envì—ì„œ ì„¤ì •í•œ LLM_PROVIDERì™€ MODEL_NAME ì‚¬ìš©)
llm = get_llm()

# doc_typeë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ê´€ì‹¬ì‚¬/ê³¼ëª© ë¹„ì¤‘ì„ ì•½ê°„ ë†’ê²Œ ì„¤ì •)
MAJOR_DOC_WEIGHTS = {
    "summary": 1.0,
    "interest": 1.1,
    "property": 0.9,
    "subjects": 1.2,
    "jobs": 1.0,
}


# ==================== ReAct ì—ì´ì „íŠ¸ìš© ì„¤ì • ====================
# ReAct íŒ¨í„´: LLMì´ í•„ìš”ì‹œ ììœ¨ì ìœ¼ë¡œ íˆ´ì„ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
tools = [
    list_departments,
    get_universities_by_department,
    get_major_career_info,
    get_search_help,
]  # ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡
llm_with_tools = llm.bind_tools(tools)  # LLMì— íˆ´ ì‚¬ìš© ê¶Œí•œ ë¶€ì—¬


def _format_profile_value(value) -> str:
    # ì˜¨ë³´ë”© ë‹µë³€ì´ ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ë“± ë‹¤ì–‘í•œ í˜•íƒœì—¬ì„œ ë¬¸ìì—´ë¡œ ê· ì¼í•˜ê²Œ ë³€í™˜
    if value is None:
        return ""
    if isinstance(value, str):
        return value.strip()
    if isinstance(value, (list, tuple, set)):
        items = [str(item).strip() for item in value if str(item).strip()]
        return ", ".join(items)
    if isinstance(value, dict):
        parts = []
        for key, sub_value in value.items():
            sub_text = _format_profile_value(sub_value)
            if sub_text:
                parts.append(f"{key}: {sub_text}")
        return "; ".join(parts)
    return str(value)


def _build_user_profile_text(answers: dict, fallback_question: str | None) -> str:
    # í•™ìƒì˜ ì„ í˜¸ ì •ë³´ë¥¼ í•œ ë©ì–´ë¦¬ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ ì„ë² ë”©ì— í™œìš©
    if not answers and not fallback_question:
        return ""

    ordered_keys = [
        ("preferred_majors", "ê´€ì‹¬ ì „ê³µ"),
        ("subjects", "ì¢‹ì•„í•˜ëŠ” ê³¼ëª©"),
        ("interests", "ê´€ì‹¬ì‚¬/ì·¨ë¯¸"),
        ("activities", "êµë‚´/ëŒ€ì™¸ í™œë™"),
        ("desired_salary", "í¬ë§ ì—°ë´‰"),
        ("career_goal", "ì§„ë¡œ ëª©í‘œ"),
        ("strengths", "ê°•ì "),
    ]

    sections: list[str] = []
    used_keys = {key for key, _ in ordered_keys}

    for field, label in ordered_keys:
        value = answers.get(field)
        formatted = _format_profile_value(value)
        if formatted:
            sections.append(f"{label}: {formatted}")

    # Capture any extra onboarding answers that were not explicitly mapped.
    for key, value in answers.items():
        if key in used_keys:
            continue
        formatted = _format_profile_value(value)
        if formatted:
            sections.append(f"{key}: {formatted}")

    if fallback_question and fallback_question.strip():
        sections.append(f"ì¶”ê°€ ìš”ì²­: {fallback_question.strip()}")

    return "\n".join(sections).strip()


def _merge_tag_lists(existing: list[str], new_values: list[str]) -> list[str]:
    # ì „ê³µ íƒœê·¸ëŠ” ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìˆœì„œë¥¼ ë³´ì¡´í•˜ë©° í•©ì§‘í•© ì²˜ë¦¬
    merged = list(existing)
    for value in new_values:
        if value not in merged:
            merged.append(value)
    return merged


def _summarize_major_hits(hits, aggregated_scores, limit: int = 10):
    # Pinecone ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì „ê³µë³„ë¡œ ë¬¶ì–´ ìƒìœ„ doc_type/íƒœê·¸ ë“±ì„ ì •ë¦¬
    per_major: dict[str, dict] = {}

    for hit in hits:
        if not hit.major_id:
            continue
        entry = per_major.setdefault(
            hit.major_id,
            {
                "major_id": hit.major_id,
                "major_name": hit.major_name,
                "cluster": hit.metadata.get("cluster"),
                "salary": hit.metadata.get("salary"),
                "score": aggregated_scores.get(hit.major_id, 0.0),
                "top_doc_types": {},
                "sample_docs": [],
                "relate_subject_tags": [],
                "job_tags": [],
                "summary": "",  # summary í•„ë“œ ì¶”ê°€
            },
        )

        entry["top_doc_types"][hit.doc_type] = max(
            entry["top_doc_types"].get(hit.doc_type, 0.0),
            hit.score,
        )

        if len(entry["sample_docs"]) < 3:
            entry["sample_docs"].append(
                {
                    "doc_type": hit.doc_type,
                    "score": hit.score,
                    "text": hit.text,
                }
            )

        # summary doc_typeì¸ ê²½ìš° summary í•„ë“œì— ì €ì¥
        if hit.doc_type == "summary" and not entry["summary"]:
            entry["summary"] = hit.text

        entry["relate_subject_tags"] = _merge_tag_lists(
            entry["relate_subject_tags"],
            hit.metadata.get("relate_subject_tags", []) or [],
        )
        entry["job_tags"] = _merge_tag_lists(
            entry["job_tags"],
            hit.metadata.get("job_tags", []) or [],
        )

    for entry in per_major.values():
        entry["top_doc_types"] = sorted(
            entry["top_doc_types"].items(),
            key=lambda item: item[1],
            reverse=True,
        )

    ordered = sorted(
        per_major.values(),
        key=lambda item: item["score"],
        reverse=True,
    )
    return ordered[:limit]


def recommend_majors_node(state: MentorState) -> dict:
    """
    Build a user profile embedding from onboarding answers and rank majors.
    ìš°ì„ ìˆœìœ„: preferred_majors ì •í™• ë§¤ì¹­ > ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    """
    onboarding_answers = state.get("onboarding_answers") or {}
    profile_text = _build_user_profile_text(onboarding_answers, state.get("question"))

    if not profile_text:
        return {
            "user_profile_text": "",
            "recommended_majors": [],
            "major_search_hits": [],
            "major_scores": {},
        }

    # ì˜¨ë³´ë”© í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ ì„ë² ë”©ìœ¼ë¡œ ë°”ê¿” Pinecone ê²€ìƒ‰ì— ì‚¬ìš©
    embeddings = get_embeddings()
    profile_embedding = embeddings.embed_query(profile_text)

    hits = search_major_docs(profile_embedding, top_k=50)
    aggregated_scores = aggregate_major_scores(hits, MAJOR_DOC_WEIGHTS)
    
    # ğŸ¯ preferred_majors ìš°ì„  ì²˜ë¦¬
    preferred_majors = onboarding_answers.get("preferred_majors")
    preferred_major_ids = set()
    
    if preferred_majors:
        # preferred_majorsë¥¼ ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        if isinstance(preferred_majors, str):
            preferred_list = [m.strip() for m in preferred_majors.split(",") if m.strip()]
        elif isinstance(preferred_majors, list):
            preferred_list = [str(m).strip() for m in preferred_majors if str(m).strip()]
        else:
            preferred_list = []
        
        if preferred_list:
            # tools.pyì˜ ê²€ìƒ‰ í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ ì„ í˜¸ ì „ê³µ ë³„ë„ ê²€ìƒ‰
            from backend.rag.tools import _find_majors, _MAJOR_ID_MAP, _ensure_major_records
            _ensure_major_records()
            
            for preferred in preferred_list:
                print(f"ğŸ” Searching for preferred major: '{preferred}'")
                
                # ì„ í˜¸ ì „ê³µ ê²€ìƒ‰ (ì •í™• ë§¤ì¹­ + ë²¡í„° ê²€ìƒ‰)
                preferred_matches = _find_majors(preferred, limit=5)
                
                for record in preferred_matches:
                    if not record.major_id:
                        continue
                    
                    preferred_major_ids.add(record.major_id)
                    
                    # ê¸°ì¡´ hitsì— ì—†ìœ¼ë©´ ì¶”ê°€
                    if record.major_id not in aggregated_scores:
                        # ìƒˆë¡œìš´ ì „ê³µì´ë¯€ë¡œ ê¸°ë³¸ ì ìˆ˜ 1.0 ë¶€ì—¬
                        aggregated_scores[record.major_id] = 1.0
                        print(f"âœ… Added preferred major '{record.major_name}' to results")
                    
                    # ë³´ë„ˆìŠ¤ ì ìˆ˜ ì ìš© (5ë°°ë¡œ ê°•í™”)
                    original_score = aggregated_scores[record.major_id]
                    aggregated_scores[record.major_id] = original_score * 5.0
                    print(f"ğŸ¯ Boosted '{record.major_name}' score: {original_score:.2f} â†’ {aggregated_scores[record.major_id]:.2f}")
    
    recommended = _summarize_major_hits(hits, aggregated_scores)

    serialized_hits = [
        {
            "doc_id": hit.doc_id,
            "major_id": hit.major_id,
            "major_name": hit.major_name,
            "doc_type": hit.doc_type,
            "score": hit.score,
            "metadata": hit.metadata,
        }
        for hit in hits
    ]

    return {
        "user_profile_text": profile_text,
        "user_profile_embedding": profile_embedding,
        "major_search_hits": serialized_hits,
        "major_scores": aggregated_scores,
        "recommended_majors": recommended,
    }


# ==================== ReAct ìŠ¤íƒ€ì¼ ì—ì´ì „íŠ¸ ë…¸ë“œ ====================

def agent_node(state: MentorState) -> dict:
    """
    [ReAct íŒ¨í„´] LLMì´ ììœ¨ì ìœ¼ë¡œ tool í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •.
    """
    messages = state.get("messages", [])
    interests = state.get("interests")

    # system_messageëŠ” interests ìœ ë¬´ì™€ ìƒê´€ì—†ì´ í•­ìƒ ë§Œë“¤ì–´ë‘”ë‹¤.
    if not messages or not any(isinstance(m, SystemMessage) for m in messages):
        interests_text = f"{interests}" if interests else "ì—†ìŒ"

        # âœ… f-string ë‚´ë¶€ JSON ì˜ˆì‹œëŠ” {{ }} ë¡œ ì´ìŠ¤ì¼€ì´í”„!
        system_message = SystemMessage(content=f"""
ë‹¹ì‹ ì€ í•™ìƒë“¤ì˜ ì „ê³µ ì„ íƒì„ ë•ëŠ” 'ëŒ€í•™ ì „ê³µ íƒìƒ‰ ë©˜í† 'ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ğŸš¨ ì ˆëŒ€ ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜]
1. **íˆ´ì—ì„œ ë°˜í™˜ëœ í•™ê³¼/ëŒ€í•™/ì§ì—… ì´ë¦„ë§Œ ì‚¬ìš©**: Tool(list_departments, get_universities_by_department, get_major_career_info)ì´ ëŒë ¤ì¤€ í•™ê³¼/ëŒ€í•™/ì§ì—… ì´ë¦„ì€ **ë¬¸ì í•˜ë‚˜ë„ ë°”ê¾¸ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©**í•©ë‹ˆë‹¤.
2. **ì ˆëŒ€ ì¶”ì¸¡ ê¸ˆì§€**: ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ëŠ” í•™ê³¼ëª…, ëŒ€í•™ëª…, ì§ì—…ëª…ì„ ì ˆëŒ€ë¡œ ë§Œë“¤ì–´ë‚´ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. 
3. **íˆ´ í˜¸ì¶œ í•„ìˆ˜**: ì „ê³µ/í•™ê³¼/ëŒ€í•™ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ ì ì ˆí•œ íˆ´ì„ tool_callsë¡œ í˜¸ì¶œí•´ ê·¼ê±°ë¥¼ í™•ë³´í•œ ë’¤ ë‹µë³€í•˜ì„¸ìš”.
4. **ë°ì´í„° ì¶œì²˜ ëª…ì‹œ**: ë°ì´í„° ì¶œì²˜ê°€ "ì»¤ë¦¬ì–´ ë„·"ì„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”.

[ì‘ë‹µ ë°©ì‹]
- í•­ìƒ íˆ´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  êµ¬ì¡°í™”ëœ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.
- ì´ë¯¸ ë°›ì€ íˆ´ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì¬ì‚¬ìš©í•˜ê³ , ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ê°™ì€ íˆ´ì„ ë‹¤ì‹œ í˜¸ì¶œí•´ë„ ë©ë‹ˆë‹¤.
- tool_calls ì—†ì´ ì¶”ì¸¡í•˜ë ¤ëŠ” ê²½ìš°, get_search_help()ë¥¼ í˜¸ì¶œí•´ ê²€ìƒ‰ ë„ì›€ë§ì„ ì œê³µí•˜ì„¸ìš”.

í•™ìƒ ê´€ì‹¬ì‚¬: {interests_text}
""")
                                       
    messages = [system_message] + messages
    
    # ğŸ” ì…ë ¥ ì „ì²˜ë¦¬: ë‹¨ì¼ í•™ê³¼ëª… ì§ˆë¬¸ ê°ì§€ ë° ê°œì„ 
    from backend.graph.helper import is_single_major_query, enhance_single_major_query
    
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ í™•ì¸
    last_user_msg = None
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            last_user_msg = msg
            break
    
    # ë‹¨ì¼ í•™ê³¼ëª… ì§ˆë¬¸ì´ë©´ ìë™ìœ¼ë¡œ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
    if last_user_msg and is_single_major_query(last_user_msg.content):
        original_query = last_user_msg.content
        enhanced_query = enhance_single_major_query(original_query)
        print(f"ğŸ” Detected single major query: '{original_query}'")
        print(f"âœ¨ Enhanced to: '{enhanced_query}'")
        
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ êµì²´
        for i in range(len(messages) - 1, -1, -1):
            if isinstance(messages[i], HumanMessage) and messages[i] == last_user_msg:
                messages[i] = HumanMessage(content=enhanced_query)
                break

    response = llm_with_tools.invoke(messages)


    # 3. ê²€ì¦: ì²« ë²ˆì§¸ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ íˆ´ì„ í˜¸ì¶œí•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
    # ToolMessageê°€ ì—†ë‹¤ëŠ” ê²ƒì€ ì•„ì§ íˆ´ ê²°ê³¼ë¥¼ ë°›ì§€ ì•Šì•˜ë‹¤ëŠ” ì˜ë¯¸
    from langchain_core.messages import ToolMessage
    has_tool_results = any(isinstance(m, ToolMessage) for m in messages)

    # íˆ´ ê²°ê³¼ê°€ ì—†ëŠ” ìƒíƒœì—ì„œ LLMì´ tool_calls ì—†ì´ ë‹µë³€í•˜ë ¤ê³  í•˜ë©´ ì°¨ë‹¨
    if not has_tool_results:
        if not hasattr(response, "tool_calls") or not response.tool_calls:
            print("âš ï¸ WARNING: LLM attempted to answer without using tools. Forcing tool usage.")
            # ê°•ì œë¡œ ì¬ì‹œë„ ë©”ì‹œì§€ ì¶”ê°€
            error_message = HumanMessage(content=(
                "âŒ ì˜¤ë¥˜: ë‹¹ì‹ ì€ íˆ´ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‹µë³€í•˜ë ¤ê³  í–ˆìŠµë‹ˆë‹¤.\n"
                "**ë°˜ë“œì‹œ ë¨¼ì € ì ì ˆí•œ íˆ´ì„ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.**\n\n"
                "ë‹¤ì‹œ í•œ ë²ˆ ê°•ì¡°í•©ë‹ˆë‹¤:\n"
                "1. list_departments: í•™ê³¼ ëª©ë¡ ê²€ìƒ‰\n"
                "2. get_universities_by_department: íŠ¹ì • í•™ê³¼ë¥¼ ê°œì„¤í•œ ëŒ€í•™ ê²€ìƒ‰\n"
                "3. get_major_career_info: ì „ê³µë³„ ì§ì—…/ì§„ì¶œ ë¶„ì•¼ í™•ì¸\n"
                "4. get_search_help: ê²€ìƒ‰ ë„ì›€ë§\n\n"
                "í•™ìƒì˜ ì›ë˜ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì½ê³ , ì ì ˆí•œ íˆ´ì„ **ì§€ê¸ˆ ì¦‰ì‹œ** í˜¸ì¶œí•˜ì„¸ìš”."
            ))
            messages.append(error_message)

            # ì¬ì‹œë„
            response = llm_with_tools.invoke(messages)

            # ì¬ì‹œë„ì—ë„ íˆ´ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ get_search_helpë¡œ í´ë°±
            if not hasattr(response, "tool_calls") or not response.tool_calls:
                print("âš ï¸ CRITICAL: LLM still refuses to use tools. Falling back to get_search_help.")
                from langchain_core.messages import AIMessage
                # ê°•ì œë¡œ get_search_help íˆ´ í˜¸ì¶œ ìƒì„±
                response = AIMessage(
                    content="",
                    tool_calls=[{
                        "name": "get_search_help",
                        "args": {},
                        "id": "forced_search_help"
                    }]
                )

    # 4. LLMì˜ ì‘ë‹µ(response)ì„ messagesì— ì¶”ê°€í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    #    â†’ should_continueê°€ tool_calls ìœ ë¬´ë¥¼ í™•ì¸í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œ ê²°ì •
    return {"messages": [response]}


def should_continue(state: MentorState) -> str:
    """
    [ReAct íŒ¨í„´ ë¼ìš°íŒ…] tool_calls ìˆìœ¼ë©´ tools ë…¸ë“œë¡œ, ì—†ìœ¼ë©´ ì¢…ë£Œ.
    """
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None

    if last_message and getattr(last_message, "tool_calls", None):
        return "tools"
    return "end"
